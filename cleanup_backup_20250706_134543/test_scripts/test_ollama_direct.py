#!/usr/bin/env python3
"""
Test Ollama provider directly to diagnose AI summarization issues
"""

import sys
import asyncio
sys.path.insert(0, 'src')

from core.llm_providers.ollama_provider import OllamaProvider
from core.llm_providers.base_provider import LLMConfig

def test_ollama_sync():
    """Test Ollama with synchronous method"""
    print("ğŸ§ª Testing Ollama Provider (Synchronous Method)...")
    
    config = LLMConfig(
        provider_name="ollama",
        enabled=True,
        base_url="http://localhost:11434",
        timeout=30
    )
    
    provider = OllamaProvider(config)
    
    # Test if Ollama is available
    print(f"ğŸ“¡ Ollama available: {provider.is_available()}")
    
    if not provider.is_available():
        print("âŒ Ollama is not available. Make sure it's running.")
        return False
    
    # Test models
    models = provider.get_models()
    print(f"ğŸ¤– Available models: {models}")
    
    if not models:
        print("âŒ No models available in Ollama")
        return False
    
    # Test summarization
    test_text = """
    BRICS nations' interest in national currency trade is not de-dollarisation, according to MEA official.
    The US dollar will continue to exist in global trade, said the official. BRICS nations will work on 
    increasing their understanding of the importance of having alternative payment systems.
    """
    
    print("ğŸ“ Testing summarization...")
    try:
        # Use the synchronous method directly
        response = provider._summarize_sync(test_text, model=models[0] if models else 'llama3:8b')
        
        print(f"âœ… Summarization successful!")
        print(f"ğŸ“„ Summary: {response.content}")
        print(f"ğŸ¤– Model: {response.model}")
        print(f"â±ï¸  Response time: {response.response_time:.2f}s")
        print(f"ğŸ¯ Success: {response.success}")
        
        return response.success
        
    except Exception as e:
        print(f"âŒ Summarization failed: {str(e)}")
        return False

async def test_ollama_async():
    """Test Ollama with async method"""
    print("\nğŸ§ª Testing Ollama Provider (Async Method)...")
    
    config = LLMConfig(
        provider_name="ollama",
        enabled=True,
        base_url="http://localhost:11434",
        timeout=30
    )
    
    provider = OllamaProvider(config)
    
    test_text = """
    BRICS nations' interest in national currency trade is not de-dollarisation, according to MEA official.
    The US dollar will continue to exist in global trade, said the official.
    """
    
    try:
        response = await provider.summarize(test_text)
        
        print(f"âœ… Async summarization successful!")
        print(f"ğŸ“„ Summary: {response.content}")
        print(f"ğŸ¯ Success: {response.success}")
        
        return response.success
        
    except Exception as e:
        print(f"âŒ Async summarization failed: {str(e)}")
        return False

if __name__ == "__main__":
    print("ğŸ”§ Testing Ollama AI Summarization\n")
    
    # Test synchronous method
    sync_success = test_ollama_sync()
    
    # Test async method
    try:
        async_success = asyncio.run(test_ollama_async())
    except Exception as e:
        print(f"âŒ Async test failed with error: {str(e)}")
        async_success = False
    
    print(f"\nğŸ“Š Test Results:")
    print(f"   Synchronous Method: {'âœ… PASS' if sync_success else 'âŒ FAIL'}")
    print(f"   Asynchronous Method: {'âœ… PASS' if async_success else 'âŒ FAIL'}")
    
    if sync_success:
        print("\nğŸ‰ Ollama AI summarization is working with synchronous method!")
    elif async_success:
        print("\nâš ï¸  Only async method works. Need to fix sync integration.")
    else:
        print("\nâŒ Both methods failed. Ollama integration needs debugging.")
