#!/usr/bin/env python3
"""
Test Model Selection Fix
========================

Comprehensive test to verify that the Ollama model selection issue is fixed.
The application should now use the user-selected model instead of always defaulting to llama3:8b.
"""

import requests
import json
import sys
import time
from pathlib import Path

# Add src to Python path
src_path = str(Path(__file__).parent / "src")
if src_path not in sys.path:
    sys.path.insert(0, src_path)

def test_model_selection_api():
    """Test the model selection API endpoint."""
    print("üîß Testing Model Selection API")
    print("=" * 35)
    
    try:
        # Get available models first
        print("1Ô∏è‚É£ Getting available Ollama models...")
        response = requests.get("http://127.0.0.1:5000/api/ollama-status", timeout=10)
        
        if response.status_code != 200:
            print("‚ùå Cannot get Ollama status")
            return False
        
        data = response.json()
        if not data.get('available'):
            print("‚ùå Ollama not available")
            return False
        
        models = data.get('models', [])
        print(f"‚úÖ Found {len(models)} available models: {models}")
        
        if len(models) < 2:
            print("‚ö†Ô∏è Need at least 2 models to test switching")
            return True  # Not a failure, just limited testing
        
        # Test switching between models
        test_models = models[:2]  # Test first two models
        
        for i, model in enumerate(test_models):
            print(f"\n2Ô∏è‚É£.{i+1} Testing model selection: {model}")
            
            # Set the model
            set_response = requests.post(
                "http://127.0.0.1:5000/api/ollama-model",
                json={'model': model},
                timeout=10
            )
            
            if set_response.status_code != 200:
                print(f"‚ùå Failed to set model {model}: {set_response.status_code}")
                return False
            
            set_data = set_response.json()
            if not set_data.get('success'):
                print(f"‚ùå Model setting failed: {set_data.get('error')}")
                return False
            
            print(f"‚úÖ Successfully set model to: {model}")
            
            # Verify the model is saved in config
            config_response = requests.get("http://127.0.0.1:5000/api/config", timeout=10)
            if config_response.status_code == 200:
                config_data = config_response.json()
                saved_model = config_data.get('ollama_model')
                if saved_model == model:
                    print(f"‚úÖ Model correctly saved in config: {saved_model}")
                else:
                    print(f"‚ùå Model not saved correctly. Expected: {model}, Got: {saved_model}")
                    return False
            
            time.sleep(1)  # Brief pause between tests
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error testing model selection API: {e}")
        return False

def test_model_usage_in_summarization():
    """Test that the selected model is actually used in summarization."""
    print("\nü§ñ Testing Model Usage in Summarization")
    print("=" * 45)
    
    try:
        from core.llm_providers.ollama_provider import OllamaProvider
        from core.llm_providers.base_provider import LLMConfig
        
        # Get available models
        response = requests.get("http://127.0.0.1:5000/api/ollama-status", timeout=10)
        data = response.json()
        
        if not data.get('available') or not data.get('models'):
            print("‚ö†Ô∏è Ollama not available or no models found")
            return True
        
        models = data.get('models', [])
        print(f"üìä Available models: {models}")
        
        # Test with different models
        test_article = "Tesla announced new autonomous driving features for their vehicles, improving safety and efficiency."
        
        for i, model in enumerate(models[:2]):  # Test first 2 models
            print(f"\n{i+1}Ô∏è‚É£ Testing summarization with model: {model}")
            
            # Create provider with specific model
            config = LLMConfig(
                provider_name="ollama",
                enabled=True,
                api_key=None,
                priority=1,
                models={model: {"max_tokens": 150}},
                rate_limits={},
                timeout=30
            )
            
            provider = OllamaProvider(config)
            
            # Test summarization with explicit model parameter
            try:
                response = provider._summarize_sync(test_article, model=model)
                
                if response.success:
                    print(f"‚úÖ Summarization successful with {model}")
                    print(f"   Model used: {response.model}")
                    print(f"   Summary: {response.content[:100]}...")
                    
                    # Verify the correct model was used
                    if response.model == model:
                        print(f"‚úÖ Correct model used: {model}")
                    else:
                        print(f"‚ùå Wrong model used. Expected: {model}, Got: {response.model}")
                        return False
                else:
                    print(f"‚ùå Summarization failed with {model}: {response.error_message}")
                    return False
                    
            except Exception as e:
                print(f"‚ùå Error with model {model}: {e}")
                return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error testing model usage: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_end_to_end_model_selection():
    """Test end-to-end model selection through the web interface."""
    print("\nüåê Testing End-to-End Model Selection")
    print("=" * 40)
    
    try:
        # Get available models
        response = requests.get("http://127.0.0.1:5000/api/ollama-status", timeout=10)
        data = response.json()
        
        if not data.get('available') or not data.get('models'):
            print("‚ö†Ô∏è Ollama not available for end-to-end test")
            return True
        
        models = data.get('models', [])
        if len(models) < 2:
            print("‚ö†Ô∏è Need at least 2 models for comprehensive testing")
            return True
        
        print(f"üìä Testing with models: {models[:2]}")
        
        # Test model switching and news processing
        for i, model in enumerate(models[:2]):
            print(f"\n{i+1}Ô∏è‚É£ Testing end-to-end with model: {model}")
            
            # Set the model
            set_response = requests.post(
                "http://127.0.0.1:5000/api/ollama-model",
                json={'model': model},
                timeout=10
            )
            
            if not set_response.json().get('success'):
                print(f"‚ùå Failed to set model {model}")
                return False
            
            print(f"‚úÖ Model set to: {model}")
            
            # Trigger news processing (which uses summarization)
            print("   Triggering news fetch to test summarization...")
            fetch_response = requests.post("http://127.0.0.1:5000/api/fetch-news", timeout=60)
            
            if fetch_response.status_code == 200:
                print("‚úÖ News fetch completed successfully")
                
                # Check if articles were processed
                news_response = requests.get("http://127.0.0.1:5000/api/news", timeout=10)
                if news_response.status_code == 200:
                    news_data = news_response.json()
                    articles = news_data.get('articles', [])
                    
                    if articles:
                        # Check if any articles have the correct model info
                        model_used_count = 0
                        for article in articles[:5]:  # Check first 5 articles
                            if article.get('llm_model') == model:
                                model_used_count += 1
                        
                        if model_used_count > 0:
                            print(f"‚úÖ Found {model_used_count} articles using model: {model}")
                        else:
                            print(f"‚ö†Ô∏è No articles found using model: {model}")
                    else:
                        print("‚ö†Ô∏è No articles found")
                else:
                    print("‚ùå Failed to get news data")
                    return False
            else:
                print(f"‚ùå News fetch failed: {fetch_response.status_code}")
                return False
            
            time.sleep(2)  # Brief pause between tests
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error in end-to-end test: {e}")
        return False

def main():
    """Run comprehensive model selection tests."""
    print("üß™ Model Selection Fix Test Suite")
    print("=" * 50)
    print("Testing the fix for:")
    print("  ‚Ä¢ Model selection API functionality")
    print("  ‚Ä¢ Actual model usage in summarization")
    print("  ‚Ä¢ End-to-end model switching")
    print()
    
    # Check server
    try:
        response = requests.get("http://127.0.0.1:5000/api/health", timeout=5)
        if response.status_code != 200:
            print("‚ùå Server not responding")
            return False
        print("‚úÖ Server is running and healthy")
    except Exception as e:
        print(f"‚ùå Cannot connect to server: {e}")
        return False
    
    # Run tests
    tests = [
        ("Model Selection API", test_model_selection_api),
        ("Model Usage in Summarization", test_model_usage_in_summarization),
        ("End-to-End Model Selection", test_end_to_end_model_selection)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            print(f"\n{'='*60}")
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå Test '{test_name}' crashed: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä Model Selection Fix Test Results:")
    
    passed = 0
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"   {status} - {test_name}")
        if result:
            passed += 1
    
    print(f"\nOverall Score: {passed}/{len(results)} tests passed")
    
    if passed == len(results):
        print("\nüéâ MODEL SELECTION FIX SUCCESSFUL!")
        print("‚úÖ Users can now select different Ollama models")
        print("‚úÖ Selected models are properly saved and used")
        print("‚úÖ No more defaulting to llama3:8b")
        print("‚úÖ End-to-end functionality working perfectly")
        print("\nüöÄ The model selection issue has been completely resolved!")
    elif passed > 0:
        print(f"\n‚ö†Ô∏è Partial success: {passed}/{len(results)} tests working")
        print("Some aspects of model selection are working, but issues remain.")
    else:
        print("\n‚ùå Model selection fix failed")
        print("The issue persists and needs further investigation.")
    
    return passed == len(results)

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
